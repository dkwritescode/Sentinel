{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41fe71fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (99999, 2)\n",
      "Time: 0.22s\n",
      "\n",
      "Extracting features (vectorized)...\n",
      "Features extracted in 2.22s\n",
      "Total time: 2.64s\n",
      "\n",
      "Features: 22\n",
      "Samples: 99999\n",
      "\n",
      "Splitting data...\n",
      "Training LightGBM...\n",
      "Training completed in 1.67s\n",
      "\n",
      "============================================================\n",
      "ACCURACY: 0.9662 (96.62%)\n",
      "============================================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.98      0.99      0.98     14652\n",
      "  defacement       0.93      0.94      0.93      3670\n",
      "     malware       0.91      0.74      0.82       486\n",
      "    phishing       0.93      0.89      0.91      1192\n",
      "\n",
      "    accuracy                           0.97     20000\n",
      "   macro avg       0.94      0.89      0.91     20000\n",
      "weighted avg       0.97      0.97      0.97     20000\n",
      "\n",
      "Confusion matrix saved\n",
      "\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Dataset size:      99,999 URLs\n",
      "Features:          22\n",
      "Accuracy:          0.9662 (96.62%)\n",
      "Training time:     1.67s\n",
      "Total time:        4.84s (0.08 min)\n",
      "Processing speed:  20682 URLs/second\n",
      "============================================================\n",
      "\n",
      "SUCCESS! Completed in 4.84 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "import time\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "start_time = time.time() #Start timer\n",
    "\n",
    "#Loading dataset\n",
    "df = pd.read_csv('data/urls.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Time: {time.time() - start_time:.2f}s\\n\")\n",
    "\n",
    "# Visualization\n",
    "sample_size = min(5000, len(df))\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='type', data=df_sample, palette='Set2')\n",
    "plt.title('Distribution of URL Types', fontsize=12)\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('url_distribution.png', dpi=80)\n",
    "plt.close()\n",
    "\n",
    "#Encoding labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['type'])\n",
    "print(\"Extracting features (vectorized)...\")\n",
    "feature_time = time.time()\n",
    "\n",
    "#Easy patterns\n",
    "suspicious_words = ['login', 'signin', 'verify', 'update', 'banking', 'account', \n",
    "                   'secure', 'paypal', 'confirm', 'password', 'admin']\n",
    "common_tlds = {'com', 'org', 'net', 'edu', 'gov', 'uk', 'us', 'in'}\n",
    "\n",
    "#Feature extraction\n",
    "urls = df['url'].astype(str)\n",
    "df['url_length'] = urls.str.len() #Length\n",
    "\n",
    "#Count operations\n",
    "df['num_dots'] = urls.str.count(r'\\.')\n",
    "df['num_slashes'] = urls.str.count('/')\n",
    "df['num_dashes'] = urls.str.count('-')\n",
    "df['num_underscores'] = urls.str.count('_')\n",
    "df['num_digits'] = urls.str.count(r'\\d')\n",
    "df['num_params'] = urls.str.count(r'\\?')\n",
    "df['num_ampersands'] = urls.str.count('&')\n",
    "df['num_equals'] = urls.str.count('=')\n",
    "df['num_at'] = urls.str.count('@')\n",
    "df['digit_ratio'] = df['num_digits'] / df['url_length'].replace(0, 1) #Ratio features\n",
    "\n",
    "#Boolean features\n",
    "df['has_https'] = urls.str.contains('https', regex=False, case=False).astype(int)\n",
    "df['has_ip'] = urls.str.contains(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', regex=True).astype(int)\n",
    "df['has_hex'] = urls.str.contains(r'%[0-9a-fA-F]{2}', regex=True).astype(int)\n",
    "df['repeated_chars'] = urls.str.contains(r'(.)\\1{3,}', regex=True).astype(int)\n",
    "df['has_double_slash'] = urls.str[8:].str.contains('//', regex=False).fillna(0).astype(int)\n",
    "\n",
    "#Suspicious words\n",
    "suspicious_pattern = '|'.join(suspicious_words)\n",
    "df['has_suspicious'] = urls.str.contains(suspicious_pattern, case=False, regex=True).astype(int)\n",
    "\n",
    "#TLD extraction\n",
    "tld_series = urls.str.extract(r'\\.([a-zA-Z]{2,})(?:[/?#]|$)', expand=False).fillna('')\n",
    "df['tld_length'] = tld_series.str.len()\n",
    "df['is_common_tld'] = tld_series.str.lower().isin(common_tlds).astype(int)\n",
    "df['num_subdomains'] = (df['num_dots'] - 1).clip(lower=0) #Subdomain count\n",
    "\n",
    "#Special characters count\n",
    "special_chars = set(string.punctuation)\n",
    "df['num_special'] = urls.apply(lambda x: sum(1 for c in x if c in special_chars))\n",
    "\n",
    "# Shortener detection\n",
    "shorteners = ['bit.ly', 'goo.gl', 'tinyurl', 't.co', 'ow.ly']\n",
    "shortener_pattern = '|'.join(shorteners)\n",
    "df['is_shortened'] = urls.str.contains(shortener_pattern, case=False, regex=True).astype(int)\n",
    "print(f\"Features extracted in {time.time() - feature_time:.2f}s\")\n",
    "print(f\"Total time: {time.time() - start_time:.2f}s\\n\")\n",
    "\n",
    "#Prepare features\n",
    "feature_cols = ['url_length', 'num_dots', 'num_slashes', 'num_dashes', 'num_underscores',\n",
    "                'num_digits', 'num_params', 'num_ampersands', 'num_equals', 'num_at',\n",
    "                'digit_ratio', 'has_https', 'has_ip', 'has_hex', 'repeated_chars',\n",
    "                'has_double_slash', 'has_suspicious', 'tld_length', 'is_common_tld',\n",
    "                'num_subdomains', 'num_special', 'is_shortened']\n",
    "X = df[feature_cols]\n",
    "y = df['label_encoded']\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\\n\")\n",
    "\n",
    "#Train-test split\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#Single best model\n",
    "print(\"Training LightGBM...\")\n",
    "train_start = time.time()\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1,\n",
    "    force_col_wise=True\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "train_time = time.time() - train_start\n",
    "\n",
    "#Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Training completed in {train_time:.2f}s\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "#Classification\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=80)\n",
    "plt.close()\n",
    "print(\"Confusion matrix saved\\n\")\n",
    "\n",
    "#Save model and label encoder\n",
    "joblib.dump(model, 'lightgbm_model.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "#Summary\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size:      {len(df):,} URLs\")\n",
    "print(f\"Features:          {len(feature_cols)}\")\n",
    "print(f\"Accuracy:          {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Training time:     {train_time:.2f}s\")\n",
    "print(f\"Total time:        {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
    "print(f\"Processing speed:  {len(df)/total_time:.0f} URLs/second\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if total_time < 60:\n",
    "    print(f\"\\nSUCCESS! Completed in {total_time:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nTook {total_time/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
